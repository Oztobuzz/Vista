{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:40:25.479312Z","iopub.status.busy":"2024-04-12T16:40:25.478634Z","iopub.status.idle":"2024-04-12T16:41:13.272051Z","shell.execute_reply":"2024-04-12T16:41:13.270604Z","shell.execute_reply.started":"2024-04-12T16:40:25.479268Z"},"trusted":true},"outputs":[],"source":["!pip install -q datasets -U\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:13.274757Z","iopub.status.busy":"2024-04-12T16:41:13.274403Z","iopub.status.idle":"2024-04-12T16:41:13.643077Z","shell.execute_reply":"2024-04-12T16:41:13.642037Z","shell.execute_reply.started":"2024-04-12T16:41:13.274726Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","API_key = user_secrets.get_secret(\"API_key\")\n","HF_token = user_secrets.get_secret(\"HF_token\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:13.644718Z","iopub.status.busy":"2024-04-12T16:41:13.644390Z","iopub.status.idle":"2024-04-12T16:41:14.785773Z","shell.execute_reply":"2024-04-12T16:41:14.784671Z","shell.execute_reply.started":"2024-04-12T16:41:13.644691Z"},"trusted":true},"outputs":[],"source":["import google.generativeai as genai"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:14.789585Z","iopub.status.busy":"2024-04-12T16:41:14.788968Z","iopub.status.idle":"2024-04-12T16:41:16.431512Z","shell.execute_reply":"2024-04-12T16:41:16.428849Z","shell.execute_reply.started":"2024-04-12T16:41:14.789546Z"},"trusted":true},"outputs":[],"source":["genai.configure(api_key=\"\")\n","!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('{HF_token}')\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:16.436337Z","iopub.status.busy":"2024-04-12T16:41:16.435166Z","iopub.status.idle":"2024-04-12T16:41:18.476056Z","shell.execute_reply":"2024-04-12T16:41:18.474823Z","shell.execute_reply.started":"2024-04-12T16:41:16.436292Z"},"trusted":true},"outputs":[],"source":["import google.generativeai as genai\n","import pandas as pd\n","from tqdm import tqdm\n","from datasets import load_dataset\n","from datasets import Dataset\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:18.497385Z","iopub.status.busy":"2024-04-12T16:41:18.496858Z","iopub.status.idle":"2024-04-12T16:41:18.510243Z","shell.execute_reply":"2024-04-12T16:41:18.509020Z","shell.execute_reply.started":"2024-04-12T16:41:18.497328Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","def run_query(query, \n","              max_output_tokens=16000, \n","              temperature=0.5):\n","    system_message = \"\"\n","    system_message += query\n","    model = genai.GenerativeModel('gemini-pro')\n","    chat = model.start_chat(history=[])\n","    try:\n","        response = chat.send_message(\n","            system_message, \n","            safety_settings={\n","                'HARM_CATEGORY_HARASSMENT': 'block_none',\n","                'HARM_CATEGORY_HATE_SPEECH': 'block_none',\n","                'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'block_none',\n","                'HARM_CATEGORY_DANGEROUS_CONTENT': 'block_none'\n","            }, \n","            generation_config=genai.types.GenerationConfig(\n","            candidate_count=1,\n","            max_output_tokens=max_output_tokens,\n","            temperature=temperature)\n","        )\n","    except Exception as e:\n","        print(e)\n","        time.sleep(10)\n","    return response.text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:41:18.512034Z","iopub.status.busy":"2024-04-12T16:41:18.511632Z","iopub.status.idle":"2024-04-12T16:41:18.522428Z","shell.execute_reply":"2024-04-12T16:41:18.521397Z","shell.execute_reply.started":"2024-04-12T16:41:18.512004Z"},"trusted":true},"outputs":[],"source":["def main():\n","  list_of_conversations = []\n","  for conv in tqdm(processing_vi_conversations):\n","    genai.configure(api_key=API_key)\n","    msgs = [x['value'] for x in conv]\n","    \n","    list_of_prompts = []\n","    for msg in msgs:  \n","      prompt = f\"\"\"Please translate the following English string to Vietnamese, preserving the same format, output just the string:\n","    English string: {msg}\n","    Translate to Vietnamese:\"\"\"\n","      list_of_prompts.append(prompt)\n","    try:\n","        \n","      tasks = []\n","      for i in range(len(list_of_prompts)):\n","        result = run_query(list_of_prompts[i])\n","        tasks.append(result)\n","        \n","      list_of_conversations.append(tasks)\n","    \n","    except Exception as e:\n","      list_of_conversations = []\n","      break\n","      \n","  return list_of_conversations \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T19:21:54.418164Z","iopub.status.busy":"2024-04-12T19:21:54.417718Z"},"trusted":true},"outputs":[],"source":["steps = 100\n","en_dataset = load_dataset(\"Lin-Chen/ShareGPT4V\", 'ShareGPT4V')\n","vi_dataset =  load_dataset(\"Oztobuzz/Processed_Vi_ShareGPT4V\", 'default')\n","\n","\n","existed_vi_ids = vi_dataset[\"train\"]['id']\n","not_existed_vi_conversations = [x for x in en_dataset[\"train\"] if x['id'] not in existed_vi_ids]  \n","print(f'number of not_existed_vi_conversations: {len(not_existed_vi_conversations)}')\n","# Start from index 0\n","not_existed_vi_conversations = not_existed_vi_conversations[0:4500] \n","print(f'number of not_existed_vi_conversations: {len(not_existed_vi_conversations)}')\n","\n","dataset_dict = {\"id\": [], \"image\": [], \"en_conversations\": [], \"vi_conversations\": []}\n","\n","for k in range(0, len(not_existed_vi_conversations), steps):  \n","  print(f\"Processing from step {k} ...\")\n","  try:\n","    processing_vi = not_existed_vi_conversations[k:k+steps]\n","    processing_vi_conversations = copy.deepcopy([x['conversations'] for x in processing_vi])\n","    vi_conversations = []\n","    list_of_conversations = main()\n","    if(len(list_of_conversations) == 0):\n","        print(f\"Step {k} is error, will move to next step\")\n","        continue\n","        \n","    for i, conv in enumerate(processing_vi_conversations):\n","        for j, msg in enumerate(conv):\n","            msg['value'] = list_of_conversations[i][j]\n","        vi_conversations.append(conv)\n","\n","\n","\n","    for i in range(0, len(vi_conversations)):\n","        image =  processing_vi[i]['image']\n","        en_conversations = processing_vi[i]['conversations']\n","        img_id =  processing_vi[i]['id']\n","        image =  processing_vi[i]['image']\n","\n","        dataset_dict['en_conversations'].append(en_conversations) \n","        dataset_dict['vi_conversations'].append(vi_conversations[i]) \n","        dataset_dict['image'].append(image) \n","        dataset_dict['id'].append(img_id) \n","\n","    #Upload to hub every 100 samples\n","    if(k % 100 == 0):\n","        first_id = dataset_dict['id'][0]\n","        print(f\"Pushing {first_id} to HF\")\n","        vi_dataset = Dataset.from_dict(dataset_dict)\n","        vi_dataset.push_to_hub(\"Oztobuzz/Processed_Vi_ShareGPT4V\", f'start_from_{first_id}', data_dir=f\"data/start_from_{first_id}\")\n","        dataset_dict = {\"id\": [], \"image\": [], \"en_conversations\": [], \"vi_conversations\": []}\n","  except Exception as e:\n","    print(e)\n","    continue"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
